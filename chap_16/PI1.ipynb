{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b60dcc-796d-4e11-a439-bcd011015ac7",
   "metadata": {},
   "source": [
    "# Production Infrastructure\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c25b79f-35e9-42ca-b080-6cf645d58823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /Users/isisromero/anaconda3/envs/MLEIA/lib/python3.11/site-packages (3.6.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/isisromero/anaconda3/envs/MLEIA/lib/python3.11/site-packages (from optuna) (1.8.1)\n",
      "Requirement already satisfied: colorlog in /Users/isisromero/anaconda3/envs/MLEIA/lib/python3.11/site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: numpy in /Users/isisromero/anaconda3/envs/MLEIA/lib/python3.11/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/isisromero/anaconda3/envs/MLEIA/lib/python3.11/site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /Users/isisromero/anaconda3/envs/MLEIA/lib/python3.11/site-packages (from optuna) (2.0.30)\n",
      "Requirement already satisfied: tqdm in /Users/isisromero/anaconda3/envs/MLEIA/lib/python3.11/site-packages (from optuna) (4.66.4)\n",
      "Requirement already satisfied: PyYAML in /Users/isisromero/anaconda3/envs/MLEIA/lib/python3.11/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /Users/isisromero/anaconda3/envs/MLEIA/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/isisromero/anaconda3/envs/MLEIA/lib/python3.11/site-packages (from sqlalchemy>=1.3.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/isisromero/anaconda3/envs/MLEIA/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018ea39a-302d-4839-baae-e9ab7d5ef9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requests\n",
    "import requests\n",
    "\n",
    "# Math\n",
    "import math\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Date & Time\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Machine Learning Flow API\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Hyperparamter Tuning\n",
    "import optuna\n",
    "\n",
    "# Collections\n",
    "from collections import namedtuple\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# Apache Spark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a007047b-478d-4b7f-97bf-368fa0d26228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngest:\n",
    "  \n",
    "  def __init__(self, url, local, source, sink, schema, database, table_name):\n",
    "    self.url = url\n",
    "    self.local = local\n",
    "    self.source = source\n",
    "    self.sink = sink\n",
    "    self.schema = schema\n",
    "    self.database = database\n",
    "    self.table_name = table_name\n",
    "    \n",
    "  def _acquire_raw(self):\n",
    "    response = requests.get(self.url, stream=True)\n",
    "    with open(self.url.split(\"/\")[-1], \"wb\") as data:\n",
    "      data.write(response.content)\n",
    "    response.close()\n",
    "    \n",
    "  def _transfer_local(self):\n",
    "    dbutils.fs.mv(self.local, self.source)\n",
    "    \n",
    "  def _read_source(self):\n",
    "    return spark.read.csv(self.source, header=True, inferSchema=False, schema=self.schema)\n",
    "  \n",
    "  def _write_source(self, data):\n",
    "    data.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").option(\"overwriteSchema\", \"true\").save(self.sink)\n",
    "    \n",
    "  @staticmethod\n",
    "  def _supplement_data(data):\n",
    "    \n",
    "    @udf(\"double\")\n",
    "    def _calculate_heat_index(t, h):\n",
    "      f = ((t * 9/5) + 32)\n",
    "      t2 = math.pow(f, 2)\n",
    "      h2 = math.pow(h, 2)\n",
    "      c = [ 0.363445176, 0.988622465, 4.777114035, -0.114037667, -0.000850208, -0.020716198, 0.000687678, 0.000274954, 0]\n",
    "      heat_index =  c[0] + (c[1]*f) + (c[2]*h) + (c[3]*f*h) + (c[4]*t2) + (c[5]*h2) + (c[6]*t2*h) + (c[7]*f*h2) + (c[8]*t2*h2)\n",
    "      return ((heat_index - 32) * 5/9)\n",
    "    \n",
    "    return (data.withColumn(\"month\", F.from_unixtime(F.unix_timestamp(F.col(\"month\"), 'MMM'), 'MM').cast(DoubleType()))\n",
    "                .withColumn(\"day\", when(F.col(\"day\") == \"sun\", 0.0).when(F.col(\"day\") == \"mon\", 1.0)\n",
    "                                  .when(F.col(\"day\") == \"tue\", 2.0).when(F.col(\"day\") == \"wed\", 3.0)\n",
    "                                  .when(F.col(\"day\") == \"thu\", 4.0).when(F.col(\"day\") == \"fri\", 5.0)\n",
    "                                  .otherwise(6.0)\n",
    "                           )\n",
    "                .withColumn(\"heat_index\", _calculate_heat_index(F.col(\"temperature\"), F.col(\"relative_humidity\")))\n",
    "           )\n",
    "  \n",
    "  def _create_table(self):\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.database};\")\n",
    "    spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.database}.{self.table_name} USING DELTA LOCATION '{self.sink}';\"\"\")\n",
    "    dbutils.fs.rm(self.source)\n",
    "  \n",
    "  def register_data(self):\n",
    "    self._acquire_raw()\n",
    "    self._transfer_local()\n",
    "    data = self._read_source()\n",
    "    supplemented = self._supplement_data(data)\n",
    "    self._write_source(supplemented)\n",
    "    self._create_table()\n",
    "    \n",
    "  def get_data(self):\n",
    "    return spark.table(f\"{self.database}.{self.table_name}\")\n",
    "  \n",
    "  def get_data_as_pandas(self):\n",
    "    return self.get_data().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77e5427-c974-424a-b79f-d9e9b2601528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathHelper:\n",
    "  def __init__(self, experiment_name):\n",
    "    self.experiment_name = experiment_name\n",
    "  def name_generator(self):\n",
    "    return f\"//{self.experiment_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c59890-2d72-40c8-a8d7-803b6790c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Registry:\n",
    "  model_name: str\n",
    "  production_version: int\n",
    "  updated: bool\n",
    "  training_time: str\n",
    "    \n",
    "class RegistryStructure:\n",
    "  def __init__(self, data):\n",
    "    self.data = data\n",
    "  def generate_row(self):\n",
    "    spark_df = spark.createDataFrame(pd.DataFrame([vars(self.data)]))\n",
    "    return (spark_df.withColumn(\"training_time\", F.to_timestamp(F.col(\"training_time\")))\n",
    "            .withColumn(\"production_version\", F.col(\"production_version\").cast(\"long\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716a4822-d277-483c-9364-526a6bb3efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegistryLogging:\n",
    "  \n",
    "  def __init__(self, database, table, delta_location, model_name, production_version, updated):\n",
    "    self.database = database\n",
    "    self.table = table\n",
    "    self.delta_location = delta_location\n",
    "    self.entry_data = Registry(model_name, production_version, updated, self._get_time())\n",
    "  \n",
    "  @classmethod\n",
    "  def _get_time(self):\n",
    "    return datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "  \n",
    "  def _check_exists(self):\n",
    "    return spark._jsparkSession.catalog().tableExists(self.database, self.table)\n",
    "  \n",
    "  def write_entry(self):\n",
    "    log_row = RegistryStructure(self.entry_data).generate_row()\n",
    "    log_row.write.format(\"delta\").mode(\"append\").save(self.delta_location)\n",
    "    if not self._check_exists():\n",
    "      spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.database}.{self.table} USING DELTA LOCATION '{self.delta_location}';\"\"\")\n",
    "    \n",
    "\n",
    "class ModelRegistration:\n",
    "  \n",
    "  def __init__(self, experiment_name, experiment_title, model_name, metric, direction):\n",
    "    self.experiment_name = experiment_name\n",
    "    self.experiment_title = experiment_title\n",
    "    self.model_name = model_name\n",
    "    self.metric = metric\n",
    "    self.direction = direction\n",
    "    self.client = MlflowClient()\n",
    "    self.experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "    \n",
    "  def _get_best_run_info(self, key):\n",
    "    run_data = mlflow.search_runs(self.experiment_id, order_by=[f\"metrics.{self.metric} {self.direction}\"])\n",
    "    return run_data.head(1)[key].values[0]\n",
    "    \n",
    "  def _get_registered_status(self):\n",
    "    return self.client.get_registered_model(name=self.experiment_title)\n",
    "    \n",
    "  def _get_current_prod(self):\n",
    "    return [x.run_id for x in self._get_registered_status().latest_versions if x.current_stage == \"Production\"][0]\n",
    "  \n",
    "  def _get_prod_version(self):\n",
    "    return int([x.version for x in self._get_registered_status().latest_versions if x.current_stage == \"Production\"][0])\n",
    "  \n",
    "  def _get_metric(self, run_id):\n",
    "    return mlflow.get_run(run_id).data.metrics.get(self.metric)\n",
    "  \n",
    "  def _find_best(self):\n",
    "    try: \n",
    "      current_prod_id = self._get_current_prod()\n",
    "      prod_metric = self._get_metric(current_prod_id)\n",
    "    except mlflow.exceptions.RestException:\n",
    "      current_prod_id = -1\n",
    "      prod_metric = 1e7\n",
    "    \n",
    "    best_id = self._get_best_run_info('run_id')\n",
    "    best_metric = self._get_metric(best_id)\n",
    "    \n",
    "    if self.direction == \"ASC\":\n",
    "      if prod_metric < best_metric:\n",
    "        return current_prod_id\n",
    "      else:\n",
    "        return best_id\n",
    "    else:\n",
    "      if prod_metric > best_metric:\n",
    "        return current_prod_id\n",
    "      else:\n",
    "        return best_id\n",
    "  \n",
    "  def _generate_artifact_path(self, run_id):\n",
    "    return f\"runs:/{run_id}/{self.model_name}\"\n",
    "    \n",
    "  def register_best(self, registration_message, logging_location, log_db, log_table):\n",
    "    best_id = self._find_best()\n",
    "    try:\n",
    "      current_prod = self._get_current_prod()\n",
    "      current_prod_version = self._get_prod_version()\n",
    "    except mlflow.exceptions.RestException:\n",
    "      current_prod = -1\n",
    "      current_prod_version = -1\n",
    "    updated = current_prod != best_id\n",
    "    \n",
    "    if updated:\n",
    "      register_new = mlflow.register_model(self._generate_artifact_path(best_id), self.experiment_title)\n",
    "      self.client.update_registered_model(name=register_new.name, \n",
    "                                          description=\"Forest Fire Prediction for the National Park\")\n",
    "      self.client.update_model_version(name=register_new.name, \n",
    "                                       version=register_new.version, \n",
    "                                       description=registration_message)\n",
    "      self.client.transition_model_version_stage(name=register_new.name, \n",
    "                                                 version=register_new.version, \n",
    "                                                 stage=\"Production\")\n",
    "      if current_prod_version > 0:\n",
    "        self.client.transition_model_version_stage(name=register_new.name,\n",
    "                                                   version=current_prod_version,\n",
    "                                                   stage=\"Archived\")\n",
    "      RegistryLogging(log_db, log_table, logging_location, self.experiment_title, int(register_new.version), updated).write_entry()\n",
    "      return \"upgraded prod\"\n",
    "    else:\n",
    "      RegistryLogging(log_db, log_table, logging_location, self.experiment_title, int(current_prod_version), updated).write_entry()\n",
    "      return \"no change\"\n",
    "    \n",
    "  def get_model_as_udf(self):\n",
    "    prod_id = self._get_current_prod()\n",
    "    artifact_uri = self._generate_artifact_path(prod_id)\n",
    "    return mlflow.pyfunc.spark_udf(spark, model_uri=artifact_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0505f0ec-a639-43ba-8d92-a0d7f98c54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep:\n",
    "    \n",
    "    def __init__(self, data, label, test_size=0.3):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.test_size = test_size\n",
    "        \n",
    "    def split_features(self):\n",
    "        Data = namedtuple('Data', 'X y')\n",
    "        X = self.data.drop([self.label], axis=1)\n",
    "        y = self.data[self.label]\n",
    "        return Data(X, y)\n",
    "    \n",
    "    def train_test_split(self, stratify_column):\n",
    "        TrainTest = namedtuple('Data', 'X_train X_test y_train y_test X y')\n",
    "        split_data = self.split_features()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(split_data.X, \n",
    "                                                            split_data.y, \n",
    "                                                            stratify=split_data.X[stratify_column],\n",
    "                                                            test_size=self.test_size\n",
    "                                                           )\n",
    "        return TrainTest(X_train, X_test, y_train, y_test, split_data.X, split_data.y)\n",
    "\n",
    "class ModelScoring:\n",
    "    \n",
    "    def __init__(self, y_test, y_pred, param_count, algorithm):\n",
    "        self.y_test = y_test\n",
    "        self.y_pred = y_pred\n",
    "        self.n = len(y_test)\n",
    "        self.param_count = param_count\n",
    "        self.algorithm = algorithm\n",
    "    \n",
    "    def _mse_calc(self):\n",
    "        return mean_squared_error(self.y_test, self.y_pred)\n",
    "    \n",
    "    def _bic(self):\n",
    "        return self.n * np.log(self._mse_calc()) + self.param_count * np.log(self.n)\n",
    "    \n",
    "    def _rmse(self):\n",
    "        return np.sqrt(self._mse_calc())\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return {'rmse': self._rmse(), 'bic': self._bic()}[self.algorithm]\n",
    "\n",
    "class Logging:\n",
    "  \n",
    "  def __init__(self, metric, model_name, run_name):\n",
    "    self.metric = metric\n",
    "    self.model_name = model_name\n",
    "    self.run_name = run_name\n",
    "    self.signature = {}\n",
    "    \n",
    "  def log_mlflow(self, trial, data, model, params):\n",
    "    if not self.signature:\n",
    "      self.signature = infer_signature(data.X_train, model.predict(data.X_train))\n",
    "    with mlflow.start_run(run_name=self.run_name):\n",
    "      mlflow.log_params(params)\n",
    "      mlflow.sklearn.log_model(model, self.model_name, signature=self.signature)\n",
    "      scores = ModelScoring(data.y_test, model.predict(data.X_test), len(params.keys()), self.metric).evaluate()\n",
    "      mlflow.log_metric(self.metric, scores)\n",
    "      mlflow.log_param(\"trial_number\", trial.number)\n",
    "    return scores\n",
    "      \n",
    "class RandomForestTuning:\n",
    "    \n",
    "    def __init__(self, data, label, stratify_column, trials, model_name, run_name, test_size=0.3, metric='rmse'):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.stratify_column = stratify_column\n",
    "        self.trials = trials\n",
    "        self.model_name = model_name\n",
    "        self.run_name = run_name\n",
    "        self.test_size = test_size\n",
    "        self.metric = metric\n",
    "    \n",
    "    @staticmethod\n",
    "    def _random_forest_model(**kwargs):\n",
    "        model = RandomForestRegressor(n_estimators=kwargs['n_estimators'],\n",
    "                                      max_depth=kwargs['max_depth'],\n",
    "                                      min_samples_split=kwargs['min_samples_split'],\n",
    "                                      min_samples_leaf=kwargs['min_samples_leaf'],\n",
    "                                      max_leaf_nodes=kwargs['max_leaf_nodes'],\n",
    "                                      min_impurity_decrease=kwargs['min_impurity_decrease'],\n",
    "                                      max_features=kwargs['max_features'],\n",
    "                                      n_jobs=-1\n",
    "                                     )\n",
    "        return model\n",
    "    \n",
    "    def _run_trial(self, trial):\n",
    "        logger = Logging(self.metric, self.model_name, self.run_name)\n",
    "        \n",
    "        splits = DataPrep(self.data, self.label, self.test_size).train_test_split(self.stratify_column)\n",
    "        params = {\n",
    "                 'n_estimators': trial.suggest_int(\"n_estimators\", 50, 2000, 10),\n",
    "                 'max_depth': trial.suggest_int(\"max_depth\", 2, 24, 1),\n",
    "                 'min_samples_split': trial.suggest_int(\"min_samples_split\", 2, 100, 1),\n",
    "                 'min_samples_leaf': trial.suggest_int(\"min_samples_leaf\", 1, 50, 1),\n",
    "                 'max_leaf_nodes': trial.suggest_int(\"max_leaf_nodes\", 4, 500, 1),\n",
    "                 'min_impurity_decrease': trial.suggest_loguniform(\"min_impurity_decrease\", 1e-22, 1e-1),\n",
    "                 'max_features': trial.suggest_categorical(\"max_features\", ['sqrt', 'log2'])\n",
    "                }\n",
    "\n",
    "        model = self._random_forest_model(**params).fit(splits.X_train, splits.y_train)\n",
    "        scores = logger.log_mlflow(trial, splits, model, params)\n",
    "        return scores\n",
    "        \n",
    "    def run(self, experiment_name):\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        trial = optuna.create_study(direction='minimize')\n",
    "        trial.optimize(self._run_trial, self.trials)\n",
    "        return trial    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e725cf2-c38a-4ddc-abd9-62df6707776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_TITLE = \"Forest_Fire_Model_5\"\n",
    "EXPERIMENT_NAME = PathHelper(EXPERIMENT_TITLE).name_generator()\n",
    "RUN_NAME = \"initial_prod_release\"\n",
    "MODEL_NAME = \"random_forest_model\"\n",
    "OPTIMIZER_ITERATIONS = 5\n",
    "TEST_PERCENTAGE = 0.5\n",
    "\n",
    "DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv\"\n",
    "LOCAL_FILE = \"file:/databricks/driver/forestfires.csv\"\n",
    "SOURCE = \"dbfs:/home/benjamin.wilson@databricks.com/demo/firedata/fire.csv\"  \n",
    "SINK = \"/home/benjamin.wilson@databricks.com/demo/fire\" \n",
    "DATABASE = \"ben_demo\" \n",
    "TABLE_NAME = \"fire_regression\"\n",
    "LOG_TABLE = f\"{TABLE_NAME}_logs\"\n",
    "LOGGING_LOCATION = f\"{SINK}_{LOG_TABLE}\"\n",
    "\n",
    "FIRE_SCHEMA = StructType([\n",
    "  StructField('x_coord', DoubleType()),\n",
    "  StructField('y_coord', DoubleType()),\n",
    "  StructField('month', StringType()),\n",
    "  StructField('day', StringType()),\n",
    "  StructField('fine_fuel_moisture_code', DoubleType()),\n",
    "  StructField('duff_moisture_code', DoubleType()),\n",
    "  StructField('drought_code', DoubleType()),\n",
    "  StructField('initial_spread_index', DoubleType()),\n",
    "  StructField('temperature', DoubleType()),\n",
    "  StructField('relative_humidity', DoubleType()),\n",
    "  StructField('windspeed', DoubleType()),\n",
    "  StructField('rain_amount', DoubleType()),\n",
    "  StructField('area', DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0efa0a0b-7860-4611-b02b-482c42c7c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_handler = DataIngest(DATA_URL, LOCAL_FILE, SOURCE, SINK, FIRE_SCHEMA, DATABASE, TABLE_NAME)\n",
    "\n",
    "# data_handler.register_data()\n",
    "# fire_data = data_handler.get_data_as_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc874c-40f2-4e56-8125-a6c1900ffe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_tune = RandomForestTuning(fire_data, 'area', 'x_coord', OPTIMIZER_ITERATIONS, MODEL_NAME, RUN_NAME, TEST_PERCENTAGE, 'rmse')\n",
    "\n",
    "trial_rf = random_forest_tune.run(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f6c94-3863-4270-8174-6131ca57821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = ModelRegistration(EXPERIMENT_NAME, EXPERIMENT_TITLE, MODEL_NAME, \"rmse\", \"ASC\")\n",
    "\n",
    "registry.register_best(\"initial run\", LOGGING_LOCATION, DATABASE, LOG_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d76c67-d33a-4872-876f-e6e76ca38e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.table(f\"{DATABASE}.{LOG_TABLE}\").orderBy(F.col(\"training_time\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570715c2-f928-4b0e-b2b9-5ef723d30058",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = \"updated_prod_release\"\n",
    "OPTIMIZER_ITERATIONS = 50\n",
    "TEST_PERCENTAGE = 0.1\n",
    "\n",
    "random_forest_tune_update = RandomForestTuning(fire_data, 'area', 'x_coord', OPTIMIZER_ITERATIONS, MODEL_NAME, RUN_NAME, TEST_PERCENTAGE, 'rmse')\n",
    "trial_rf_update = random_forest_tune_update.run(EXPERIMENT_NAME)\n",
    "\n",
    "registry_update = ModelRegistration(EXPERIMENT_NAME, EXPERIMENT_TITLE, MODEL_NAME, \"rmse\", \"ASC\")\n",
    "registry_update.register_best(\"initial run\", LOGGING_LOCATION, DATABASE, LOG_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf14a1f-e741-4283-bc10-61a279a9998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.table(f\"{DATABASE}.{LOG_TABLE}\").orderBy(F.col(\"training_time\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e6caea-8e19-4d88-953c-71d593f38a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = \"passive_retrain_attempt\"\n",
    "OPTIMIZER_ITERATIONS = 5 \n",
    "TEST_PERCENTAGE = 0.6 \n",
    "\n",
    "random_forest_tune_update = RandomForestTuning(fire_data, 'area', 'x_coord', OPTIMIZER_ITERATIONS, MODEL_NAME, RUN_NAME, TEST_PERCENTAGE, 'rmse')\n",
    "trial_rf_update = random_forest_tune_update.run(EXPERIMENT_NAME)\n",
    "\n",
    "registry_update = ModelRegistration(EXPERIMENT_NAME, EXPERIMENT_TITLE, MODEL_NAME, \"rmse\", \"ASC\")\n",
    "registry_update.register_best(\"initial run\", LOGGING_LOCATION, DATABASE, LOG_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38035ed3-2d41-463c-8673-34d339c4ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.table(f\"{DATABASE}.{LOG_TABLE}\").orderBy(F.col(\"training_time\")))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20321e2b-1b99-484c-9743-b3da6f857315",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = spark.table(\"ben_demo.fire_regression\")\n",
    "predicted_data = raw_data.withColumn(\"prediction\", registry.get_model_as_udf()(*[x for x in raw_data.columns if x != \"area\"]))\n",
    "display(predicted_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
